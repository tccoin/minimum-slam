{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual SLAM Trilogy\n",
    "## Part II: The Backend\n",
    "\n",
    "In this SLAM hands-on lecture, we will implement a visual SLAM system that has an OpenCV frontend, a GTSAM backend, and a loop closure module based on a bag-of-words approach.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The overview of our SLAM system is depicted below, simplified by certain assumptions:\n",
    "\n",
    "1. Odometry\n",
    "    - Assumes there is an odometry trajectory provided to our SLAM system.\n",
    "    - In practice, this includes a Kalman filter fusing the IMU and encoder data.\n",
    "2. Frontend:\n",
    "    - Processes the raw sensor data and extracts relevant features for optimization.\n",
    "    - Associates each measurement to a specific landmark (3D point).\n",
    "    - Provide initial values for the backend variables.\n",
    "3. Mapping\n",
    "    - Utilizes a very minimum sparse map.\n",
    "    - Could be replaced with OGM or even 3D Gaussian Splatting in the future.\n",
    "4. Backend\n",
    "    - Solve the maximum a posteriori (MAP) estimation problem.\n",
    "    - Feed back information to loop closure.\n",
    "5. Loop closure:\n",
    "    - Acts as a long-term tracking module (compared to the short-term tracking module in frontend).\n",
    "    - Implemented with visual bag-of-word algorithm.\n",
    "\n",
    "![slam_overview](assets/slam_overview.png)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the abandoned_factory P006 sequence from the TartanAir dataset to test the system. It is a simulation dataset with diverse environments and ground truth dataset, which make it perfect for testing and evaluating our system. To get started, we'll need to access the camera intrinsics, extrinsics, and data format information, which can be found here: https://github.com/castacks/tartanair_tools/blob/master/data_type.md.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "In this notebook, we will walk through the implementation of the camera model, backend, and loop closure step-by-step, while visualizing the output of each step. Specifically, we will cover the following topics:\n",
    "\n",
    "- Calculating global XYZ positions for the landmarks\n",
    "- Optimize the trajectory with projection factors in GTSAM\n",
    "- Improve estimated trajectory by adding loop closure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Dependency\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the minslam package in “editable” mode\n",
    "# !pip install -e ..\n",
    "\n",
    "# install other libraries\n",
    "# !pip install numpy spatialmath-python opencv-python matplotlib gtsam ipympl evo plotly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import libraries and dataset\n",
    "Please download [abadoned_factory P006 dataset](https://drive.google.com/file/d/1Q_fSI0U-IMfv90lyE1Uh78KV2QJheHbv/view?usp=share_link) and extract it to a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block should run without error\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "\n",
    "# change the perspective of 3d plots with this command\n",
    "%matplotlib widget\n",
    "\n",
    "# test if we can find the dataset\n",
    "dataset_folder = '../data/P006/'\n",
    "print('Check if the data folder exists:',os.path.exists(dataset_folder))\n",
    "\n",
    "# visualization\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# frontend\n",
    "import numpy as np\n",
    "from spatialmath import *\n",
    "\n",
    "# backend\n",
    "import gtsam\n",
    "from gtsam.symbol_shorthand import L, X\n",
    "\n",
    "# our slam implementation\n",
    "from minslam.data_loader import TartanAirLoader, plot_trajectory\n",
    "from minslam.frontend import Frontend\n",
    "from minslam.params import Params\n",
    "from minslam.backend import Backend\n",
    "from minslam.camera import PinholeCamera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load images and trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_filename = 'pose_left.txt'\n",
    "traj_path = os.path.join(dataset_folder, traj_filename)\n",
    "print('Loading trajectory from ', traj_path)\n",
    "\n",
    "# load a trajectory\n",
    "dataset = TartanAirLoader('../data/P006')\n",
    "dataset.load_ground_truth()\n",
    "gt_poses = dataset.gt\n",
    "start_index = 250\n",
    "odom_poses = dataset.add_noise(gt_poses, [3e-4, 3e-4], [1e-3, 3e-4], seed=100, start=start_index)\n",
    "dataset.set_odometry(odom_poses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Frontend\n",
    "\n",
    "We will build a function that tries to find the next keyframe and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "\n",
    "def run_frontend_once(frontend):\n",
    "    pose = dataset.read_current_odometry()\n",
    "    while not frontend.keyframe_selection(pose):\n",
    "        if not dataset.load_next_frame():\n",
    "            break\n",
    "        pose = dataset.read_current_odometry()\n",
    "    color, depth = dataset.read_current_rgbd()\n",
    "    frontend.add_keyframe(pose, color, depth, dataset.curr_index)\n",
    "    print(f'--- Added keyframe {frontend.frame_id} (seq id: {dataset.curr_index}) ---')\n",
    "    more_points_n = params['frontend']['feature']['number']\n",
    "    frontend.extract_features(more_points_n, append_mode=False)\n",
    "    print('extracting features:', len(frontend.curr_frame.keypoints), f'(expected {more_points_n})')\n",
    "    if frontend.frame_id > 0:\n",
    "        frontend.match_features()\n",
    "        print('matching features:', len(frontend.curr_frame.matches))\n",
    "        frontend.eliminate_outliers()\n",
    "    frontend.assign_global_id()\n",
    "    # frontend.plot_matches(with_global_id=True)\n",
    "    return frontend.curr_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Camera\n",
    "\n",
    "In this section, we will learn what is pinhole camera model and how to use it to calculate the XYZ global position of the landmarks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pinhole camera model\n",
    "\n",
    "To help us understand the pinhole camera model, we recommend reviewing two resources:\n",
    "\n",
    "- [CS231A Course Notes 1: Camera Models](https://web.stanford.edu/class/cs231a/course_notes/01-camera-models.pdf): how to derive the pinhole camera model.\n",
    "- [OpenCV - Camera Calibration and 3D Reconstruction](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html): how a OpenCV library models and implements pinhole camera.\n",
    "\n",
    "These resources should provide a good foundation for understanding the principles behind the pinhole camera model and how it is implemented in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = PinholeCamera(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate the global XYZ positions of landmarks\n",
    "\n",
    "Next, we will use the camera intrinsics and the pixel coordinates of each feature to calculate its 3D position in the camera frame. We will then use the camera poses and the 3D positions of the features to calculate the global XYZ positions of the landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_curr_index(100)\n",
    "frontend_frame = run_frontend_once(frontend)\n",
    "color = frontend_frame.color\n",
    "depth = frontend_frame.depth\n",
    "\n",
    "\n",
    "fig_color = px.imshow(color[:,:,::-1])\n",
    "fig_color.update_traces(hoverinfo=\"x+y+z\", name=\"\")\n",
    "fig_color.show()\n",
    "\n",
    "clipped_depth = depth.clip(0, 40)\n",
    "fig_depth = px.imshow(clipped_depth, color_continuous_scale='gray')\n",
    "fig_depth.update_traces(hoverinfo=\"x+y+z\", name=\"\")\n",
    "fig_depth.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the global positions, we will first use the camera model to get the XYZ position of the landmarks in the camera frame (x->right, y->downward, z->forward). The point cloud below is generated by evenly sampled points from the depth image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stride = 1\n",
    "sample_number = int(640*480/sample_stride/sample_stride)\n",
    "points_xyz = np.zeros((sample_number+1, 3)) # add one row for the origin\n",
    "points_color = np.zeros((sample_number+1, 3))\n",
    "index = 0\n",
    "for i in range(0,640,sample_stride):\n",
    "    for j in range(0,480,sample_stride):\n",
    "        depth = frontend_frame.depth[j, i]\n",
    "        # skip points with large depth for better visualization\n",
    "        if depth>50:\n",
    "            continue\n",
    "        ############## transform these points from 2d to 3d ##############\n",
    "        points_xyz[index] = camera.back_project2(i, j, depth).flatten()\n",
    "        ##################################################################\n",
    "        points_color[index] = frontend_frame.color[j, i][::-1]\n",
    "        index += 1\n",
    "points_xyz = points_xyz[:index+1]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_xyz[:,0],\n",
    "    y=points_xyz[:,2],\n",
    "    z=points_xyz[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=points_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='X',\n",
    "    scene_yaxis_title='Z',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real backend, we will only use the detected sparse feature, and they look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_curr_index(100)\n",
    "frontend_frame = run_frontend_once(frontend)\n",
    "points_xyz = np.zeros((len(frontend_frame.points)+1, 3))\n",
    "points_color = np.zeros((len(frontend_frame.points)+1, 3))\n",
    "index = 0\n",
    "for point in frontend_frame.points:\n",
    "    i,j = [int(x) for x in point]\n",
    "    depth = frontend_frame.depth[j, i]\n",
    "    # skip points with large depth for better visualization\n",
    "    if depth>50:\n",
    "        continue\n",
    "    ############## transform these points from 2d to 3d ##############\n",
    "    points_xyz[index] = camera.back_project2(i, j, depth).flatten()\n",
    "    ##################################################################\n",
    "    points_color[index] = frontend_frame.color[j, i][::-1]\n",
    "    index += 1\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_xyz[:,0],\n",
    "    y=points_xyz[:,2],\n",
    "    z=points_xyz[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=points_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='X',\n",
    "    scene_yaxis_title='Z',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the features are sparse, it is not suitable for visualization. So, we will continue to use sampled points in next step.\n",
    "\n",
    "In this step, we will transform these points from camera frame to the world frame using the odometry pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stride = 1\n",
    "sample_number = int(640*480/sample_stride/sample_stride)\n",
    "points_xyz = np.zeros((sample_number+1, 3)) # add one row for the origin\n",
    "points_color = np.zeros((sample_number+1, 3))\n",
    "index = 0\n",
    "for i in range(0,640,sample_stride):\n",
    "    for j in range(0,480,sample_stride):\n",
    "        depth = frontend_frame.depth[j, i]\n",
    "        # skip points with large depth for better visualization\n",
    "        if depth>50:\n",
    "            continue\n",
    "        ############# transform these points from 2d to 3d ############\n",
    "        points_xyz[index] = camera.back_project(i, j, depth, frontend_frame.odom_pose).flatten()\n",
    "        ################################################################\n",
    "        points_color[index] = frontend_frame.color[j, i][::-1]\n",
    "        index += 1\n",
    "points_xyz = points_xyz[:index+1]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_xyz[:,2],\n",
    "    y=points_xyz[:,0],\n",
    "    z=points_xyz[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=points_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='Z',\n",
    "    scene_yaxis_title='X',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very familiar to the previous plot... So why do we want to transform it into the world frame? First, it enables us to provide an initial guess for the landmarks in backend. Secondly, we can simply stack these point cloud together to check the odometry poses. For example, run this block to see what happens if we have a bad or even wrong odometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TartanAirLoader('../data/P006/')\n",
    "dataset.load_ground_truth()\n",
    "gt_poses = dataset.gt\n",
    "\n",
    "# uncomment this part to use gt\n",
    "dataset.set_odometry(gt_poses)\n",
    "\n",
    "# uncomment this part to add small noise\n",
    "# odom_poses = dataset.add_noise(gt_poses, [1e-3, 3e-3], [1e-3, 1e-3], seed=100, start=start_index)\n",
    "# dataset.set_odometry(odom_poses)\n",
    "\n",
    "# uncomment this part to use wrong odometry\n",
    "# T = SE3([\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 0, 1, 0],\n",
    "#     [0, 1, 0, 0],\n",
    "#     [0, 0, 0, 1]\n",
    "# ])\n",
    "# odom_poses = []\n",
    "# for i in range(1, len(gt_poses)):\n",
    "#     odom_poses += [T*gt_poses[0]]\n",
    "# dataset.set_odometry(odom_poses)\n",
    "\n",
    "def get_world_points(frame_id, sample_stride=5):\n",
    "    dataset.set_curr_index(frame_id)\n",
    "    frontend_frame = run_frontend_once(frontend)\n",
    "    color = frontend_frame.color\n",
    "    depth = frontend_frame.depth\n",
    "    sample_number = int(640*480/sample_stride/sample_stride)\n",
    "    points_xyz = np.zeros((sample_number+1, 3))\n",
    "    points_color = np.zeros((sample_number+1, 3))[::-1]\n",
    "    index = 0\n",
    "    for i in range(0,640,sample_stride):\n",
    "        for j in range(0,480,sample_stride):\n",
    "            depth = frontend_frame.depth[j, i]\n",
    "            if depth>50:\n",
    "                continue\n",
    "            points_xyz[index] = camera.back_project(i, j, depth, dataset.read_current_odometry()).flatten()\n",
    "            points_color[index] = frontend_frame.color[j, i]\n",
    "            index += 1\n",
    "    return points_xyz[:index+1], points_color[:index+1]\n",
    "\n",
    "all_points = [get_world_points(i) for i in range(100, 300, 10)]\n",
    "stacked_point_cloud = np.vstack([x[0] for x in all_points])\n",
    "stacked_point_cloud_color = np.vstack([x[1] for x in all_points])\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=stacked_point_cloud[:,2],\n",
    "    y=stacked_point_cloud[:,0],\n",
    "    z=stacked_point_cloud[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=stacked_point_cloud_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='Z',\n",
    "    scene_yaxis_title='X',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Backend\n",
    "\n",
    "In this section, we will use GTSAM library to optimize the trajectory using the measurements obtained from the frontend. To incorporate the camera pose and landmark positions in the optimization process, we will use the projection factor in GTSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "params = Params('../params/tartanair.yaml')\n",
    "dataset = TartanAirLoader('../data/P006/')\n",
    "frontend = Frontend(params)\n",
    "backend = Backend(params)\n",
    "dataset.set_curr_index(start_index)\n",
    "n_keyframes = 200\n",
    "\n",
    "# read the ground truth and odometry\n",
    "dataset.load_ground_truth()\n",
    "gt_poses = dataset.gt\n",
    "odom_poses = dataset.add_noise(gt_poses, [1e-4, 3e-5], [1e-3, 3e-4], seed=100, start=start_index)\n",
    "dataset.set_odometry(odom_poses)\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_keyframes, 3))\n",
    "odom_traj = np.zeros((n_keyframes, 3))\n",
    "\n",
    "# run the frontend and backend\n",
    "for i in range(n_keyframes):\n",
    "    # get results from frontend\n",
    "    frontend_frame = run_frontend_once(frontend)\n",
    "    gt_traj[i] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i] = dataset.read_current_odometry().t\n",
    "    measurements = []\n",
    "    frame_id = frontend_frame.frame_id\n",
    "    count = 0\n",
    "    for landmark in frontend_frame.landmarks:\n",
    "        global_id = landmark.global_id\n",
    "        measurement = landmark.measurements[frame_id] # u, v, depth\n",
    "        measurements.append((global_id, *measurement))\n",
    "        count += 1\n",
    "    print(f'add {count} measurements to backend')\n",
    "    # add measurements to backend\n",
    "    backend.add_keyframe(frame_id, frontend_frame.odom_pose, measurements)\n",
    "# optimize the backend\n",
    "backend.optimize(optimizer='LM')\n",
    "backend_estimate = backend.current_estimate\n",
    "estimated_traj = gtsam.utilities.extractPose3(backend_estimate)[:, -3:]\n",
    "\n",
    "# plot the results\n",
    "fig = go.Figure()\n",
    "plot_trajectory(odom_traj[:n_keyframes], 'odom', fig)\n",
    "plot_trajectory(gt_traj[:n_keyframes], 'gt', fig)\n",
    "plot_trajectory(estimated_traj[:n_keyframes], 'estimated', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset.set_curr_index(start_index)\n",
    "\n",
    "# create frontend and backend\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "backend = Backend(params)\n",
    "n_keyframes = 100\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_keyframes, 3))\n",
    "odom_traj = np.zeros((n_keyframes, 3))\n",
    "\n",
    "# parameters for the backend\n",
    "initial_estimate = gtsam.Values()\n",
    "graph = gtsam.NonlinearFactorGraph()\n",
    "pose_prior_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.03]*3+[0.01]*3))\n",
    "point_noise = gtsam.noiseModel.Isotropic.Sigma(3, 0.3)\n",
    "noise_model = gtsam.noiseModel.Isotropic.Sigma(2, 1)\n",
    "noise_model_robust = gtsam.noiseModel.Robust.Create(gtsam.noiseModel.mEstimator.Huber.Create(1.345), noise_model)\n",
    "fx, fy, cx, cy = camera.camera_matrix\n",
    "K = gtsam.Cal3_S2(fx, fy, 0.0, cx, cy)\n",
    "\n",
    "total_measurements = 0\n",
    "\n",
    "# run the frontend and backend\n",
    "for i in range(n_keyframes):\n",
    "    # get results from frontend\n",
    "    frontend_frame = run_frontend_once(frontend)\n",
    "    gt_traj[i] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i] = dataset.read_current_odometry().t\n",
    "    measurements = []\n",
    "    frame_id = frontend_frame.frame_id\n",
    "    for landmark in frontend_frame.landmarks:\n",
    "        global_id = landmark.global_id\n",
    "        measurement = landmark.measurements[frame_id] # u, v, depth\n",
    "        measurements.append((global_id, *measurement))\n",
    "\n",
    "    # add measurements to backend\n",
    "    odom_pose = frontend_frame.odom_pose\n",
    "    pose = gtsam.Pose3(gtsam.Rot3(odom_pose.R), gtsam.Point3(odom_pose.t))\n",
    "    # add initial estimate\n",
    "    initial_estimate.insert(X(frame_id), pose)\n",
    "    if i == 0:\n",
    "        # add prior for first frame\n",
    "        graph.add(gtsam.PriorFactorPose3(X(frame_id), pose, pose_prior_noise))\n",
    "    count = 0\n",
    "    for global_id, u, v, depth in measurements:\n",
    "        # add measurements\n",
    "        global_pos = camera.back_project(u, v, depth, odom_pose).flatten()\n",
    "        if initial_estimate.exists(L(global_id)):\n",
    "            init_pos = initial_estimate.atVector(L(global_id))\n",
    "            # calc error\n",
    "            error = np.linalg.norm(init_pos - global_pos)\n",
    "            if error>0.5:\n",
    "                continue\n",
    "        graph.add(gtsam.GenericProjectionFactorCal3_S2(\n",
    "            np.array([u, v]),\n",
    "            noise_model_robust,\n",
    "            X(frame_id),\n",
    "            L(global_id),\n",
    "            K\n",
    "        ))\n",
    "        count += 1\n",
    "        if total_measurements<50:\n",
    "            total_measurements += 1\n",
    "            graph.add(gtsam.PriorFactorPoint3(L(global_id), global_pos, point_noise))\n",
    "        if not initial_estimate.exists(L(global_id)):\n",
    "            initial_estimate.insert(L(global_id), global_pos)\n",
    "    print(f'add {count} measurements to backend')\n",
    "# optimize\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial_estimate)\n",
    "current_estimate = optimizer.optimize()\n",
    "estimated_traj = gtsam.utilities.extractPose3(current_estimate)[:, -3:]\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig = go.Figure()\n",
    "plot_trajectory(odom_traj[:n_keyframes], 'odom', fig)\n",
    "plot_trajectory(gt_traj[:n_keyframes], 'gt', fig)\n",
    "plot_trajectory(estimated_traj[:n_keyframes], 'estimated', fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

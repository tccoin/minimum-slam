{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual SLAM Trilogy\n",
    "## Part II: The Backend\n",
    "\n",
    "In this SLAM hands-on lecture, we will implement a visual SLAM system that has an OpenCV frontend, a GTSAM backend, and a loop closure module based on a bag-of-words approach.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The overview of our SLAM system is depicted below, simplified by certain assumptions:\n",
    "\n",
    "1. Odometry\n",
    "    - Assumes there is an odometry trajectory provided to our SLAM system.\n",
    "    - In practice, this includes a Kalman filter fusing the IMU and encoder data.\n",
    "2. Frontend:\n",
    "    - Processes the raw sensor data and extracts relevant features for optimization.\n",
    "    - Associates each measurement to a specific landmark (3D point).\n",
    "    - Provide initial values for the backend variables.\n",
    "3. Mapping\n",
    "    - Utilizes a very minimum sparse map.\n",
    "    - Could be replaced with OGM or even 3D Gaussian Splatting in the future.\n",
    "4. Backend\n",
    "    - Solve the maximum a posteriori (MAP) estimation problem.\n",
    "    - Feed back information to loop closure.\n",
    "5. Loop closure:\n",
    "    - Acts as a long-term tracking module (compared to the short-term tracking module in frontend).\n",
    "    - Implemented with visual bag-of-word algorithm.\n",
    "\n",
    "![slam_overview](assets/slam_overview.png)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the abandoned_factory P006 sequence from the TartanAir dataset to test the system. It is a simulation dataset with diverse environments and ground truth dataset, which make it perfect for testing and evaluating our system. To get started, we'll need to access the camera intrinsics, extrinsics, and data format information, which can be found here: https://github.com/castacks/tartanair_tools/blob/master/data_type.md.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "In this notebook, we will walk through the implementation of the camera model, backend, and loop closure step-by-step, while visualizing the output of each step. Specifically, we will cover the following topics:\n",
    "\n",
    "- Calculating global XYZ positions for the landmarks\n",
    "- Optimize the trajectory with projection factors in GTSAM\n",
    "- Improve estimated trajectory by adding loop closure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Dependency\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the minslam package in “editable” mode\n",
    "# !pip install -e ..\n",
    "\n",
    "# install other libraries\n",
    "# !pip install numpy spatialmath-python opencv-python matplotlib gtsam ipympl evo plotly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import libraries and dataset\n",
    "Please download [abadoned_factory P006 dataset](https://drive.google.com/file/d/1Q_fSI0U-IMfv90lyE1Uh78KV2QJheHbv/view?usp=share_link) and extract it to a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block should run without error\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "\n",
    "# change the perspective of 3d plots with this command\n",
    "%matplotlib widget\n",
    "\n",
    "# test if we can find the dataset\n",
    "dataset_folder = '../data/P006/'\n",
    "print('Check if the data folder exists:',os.path.exists(dataset_folder))\n",
    "\n",
    "# visualization\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# frontend\n",
    "import numpy as np\n",
    "from spatialmath import *\n",
    "\n",
    "# backend\n",
    "import gtsam\n",
    "from gtsam.symbol_shorthand import L, X\n",
    "\n",
    "# our slam implementation\n",
    "from minslam.data_loader import TartanAirLoader, plot_trajectory\n",
    "from minslam.frontend import Frontend\n",
    "from minslam.params import Params\n",
    "from minslam.backend import Backend\n",
    "from minslam.camera import PinholeCamera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load images and trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_filename = 'pose_left.txt'\n",
    "traj_path = os.path.join(dataset_folder, traj_filename)\n",
    "print('Loading trajectory from ', traj_path)\n",
    "\n",
    "# load a trajectory\n",
    "dataset = TartanAirLoader('../data/P006')\n",
    "dataset.load_ground_truth()\n",
    "gt_poses = dataset.gt\n",
    "start_index = 250\n",
    "odom_poses = dataset.add_noise(gt_poses, [3e-4, 3e-4], [1e-3, 3e-4], seed=100, start=start_index)\n",
    "dataset.set_odometry(odom_poses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Frontend\n",
    "\n",
    "We will build a function that tries to find the next keyframe and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "\n",
    "def run_frontend_once(frontend):\n",
    "    pose = dataset.read_current_odometry()\n",
    "    while not frontend.keyframe_selection(pose):\n",
    "        if not dataset.load_next_frame():\n",
    "            break\n",
    "        pose = dataset.read_current_odometry()\n",
    "    color, depth = dataset.read_current_rgbd()\n",
    "    frontend.add_keyframe(pose, color, depth, dataset.curr_index)\n",
    "    print(f'--- Added keyframe {frontend.frame_id} (seq id: {dataset.curr_index}) ---')\n",
    "    more_points_n = params['frontend']['feature']['number']\n",
    "    frontend.extract_features(more_points_n, append_mode=False)\n",
    "    print('extracting features:', len(frontend.curr_frame.keypoints), f'(expected {more_points_n})')\n",
    "    if frontend.frame_id > 0:\n",
    "        frontend.match_features()\n",
    "        print('matching features:', len(frontend.curr_frame.matches))\n",
    "        frontend.eliminate_outliers()\n",
    "    frontend.assign_global_id()\n",
    "    # frontend.plot_matches(with_global_id=True)\n",
    "    return frontend.curr_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Camera\n",
    "\n",
    "In this section, we will learn what is pinhole camera model and how to use it to calculate the XYZ global position of the landmarks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pinhole camera model\n",
    "\n",
    "To help us understand the pinhole camera model, we recommend reviewing two resources:\n",
    "\n",
    "- [CS231A Course Notes 1: Camera Models](https://web.stanford.edu/class/cs231a/course_notes/01-camera-models.pdf): how to derive the pinhole camera model.\n",
    "- [OpenCV - Camera Calibration and 3D Reconstruction](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html): how a OpenCV library models and implements pinhole camera.\n",
    "\n",
    "These resources should provide a good foundation for understanding the principles behind the pinhole camera model and how it is implemented in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = PinholeCamera(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate the global XYZ positions of landmarks\n",
    "\n",
    "Next, we will use the camera intrinsics and the pixel coordinates of each feature to calculate its 3D position in the camera frame. We will then use the camera poses and the 3D positions of the features to calculate the global XYZ positions of the landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_curr_index(100)\n",
    "frontend_frame = run_frontend_once(frontend)\n",
    "color = frontend_frame.color\n",
    "depth = frontend_frame.depth\n",
    "\n",
    "\n",
    "fig_color = px.imshow(color[:,:,::-1])\n",
    "fig_color.update_traces(hoverinfo=\"x+y+z\", name=\"\")\n",
    "fig_color.show()\n",
    "\n",
    "clipped_depth = depth.clip(0, 40)\n",
    "fig_depth = px.imshow(clipped_depth, color_continuous_scale='gray')\n",
    "fig_depth.update_traces(hoverinfo=\"x+y+z\", name=\"\")\n",
    "fig_depth.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the global positions, we will first use the camera model to get the XYZ position of the landmarks in the camera frame (x->right, y->downward, z->forward). The point cloud below is generated by evenly sampled points from the depth image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stride = 1\n",
    "sample_number = int(640*480/sample_stride/sample_stride)\n",
    "points_xyz = np.zeros((sample_number+1, 3)) # add one row for the origin\n",
    "points_color = np.zeros((sample_number+1, 3))\n",
    "index = 0\n",
    "for i in range(0,640,sample_stride):\n",
    "    for j in range(0,480,sample_stride):\n",
    "        depth = frontend_frame.depth[j, i]\n",
    "        # skip points with large depth for better visualization\n",
    "        if depth>50:\n",
    "            continue\n",
    "        ############## transform these points from 2d to 3d ##############\n",
    "        points_xyz[index] = camera.back_project2(i, j, depth).flatten()\n",
    "        ##################################################################\n",
    "        points_color[index] = frontend_frame.color[j, i][::-1]\n",
    "        index += 1\n",
    "points_xyz = points_xyz[:index+1]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_xyz[:,0],\n",
    "    y=points_xyz[:,2],\n",
    "    z=points_xyz[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=points_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='X',\n",
    "    scene_yaxis_title='Z',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real backend, we will only use the detected sparse feature, and they look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_curr_index(100)\n",
    "frontend_frame = run_frontend_once(frontend)\n",
    "points_xyz = np.zeros((len(frontend_frame.points)+1, 3))\n",
    "points_color = np.zeros((len(frontend_frame.points)+1, 3))\n",
    "index = 0\n",
    "for point in frontend_frame.points:\n",
    "    i,j = [int(x) for x in point]\n",
    "    depth = frontend_frame.depth[j, i]\n",
    "    # skip points with large depth for better visualization\n",
    "    if depth>50:\n",
    "        continue\n",
    "    ############## transform these points from 2d to 3d ##############\n",
    "    points_xyz[index] = camera.back_project2(i, j, depth).flatten()\n",
    "    ##################################################################\n",
    "    points_color[index] = frontend_frame.color[j, i][::-1]\n",
    "    index += 1\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_xyz[:,0],\n",
    "    y=points_xyz[:,2],\n",
    "    z=points_xyz[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=points_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='X',\n",
    "    scene_yaxis_title='Z',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the features are sparse, it is not suitable for visualization. So, we will continue to use sampled points in next step.\n",
    "\n",
    "In this step, we will transform these points from camera frame to the world frame using the odometry pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_stride = 1\n",
    "sample_number = int(640*480/sample_stride/sample_stride)\n",
    "points_xyz = np.zeros((sample_number+1, 3)) # add one row for the origin\n",
    "points_color = np.zeros((sample_number+1, 3))\n",
    "index = 0\n",
    "for i in range(0,640,sample_stride):\n",
    "    for j in range(0,480,sample_stride):\n",
    "        depth = frontend_frame.depth[j, i]\n",
    "        # skip points with large depth for better visualization\n",
    "        if depth>50:\n",
    "            continue\n",
    "        ############# transform these points from 2d to 3d ############\n",
    "        points_xyz[index] = camera.back_project(i, j, depth, frontend_frame.odom_pose).flatten()\n",
    "        ################################################################\n",
    "        points_color[index] = frontend_frame.color[j, i][::-1]\n",
    "        index += 1\n",
    "points_xyz = points_xyz[:index+1]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=points_xyz[:,2],\n",
    "    y=points_xyz[:,0],\n",
    "    z=points_xyz[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=points_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='Z',\n",
    "    scene_yaxis_title='X',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very familiar to the previous plot... So why do we want to transform it into the world frame? First, it enables us to provide an initial guess for the landmarks in backend. Secondly, we can simply stack these point cloud together to check the odometry poses. For example, run this block to see what happens if we have a bad or even wrong odometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TartanAirLoader('../data/P006/')\n",
    "dataset.load_ground_truth()\n",
    "gt_poses = dataset.gt\n",
    "\n",
    "# uncomment this part to use gt\n",
    "dataset.set_odometry(gt_poses)\n",
    "\n",
    "# uncomment this part to add small noise\n",
    "# odom_poses = dataset.add_noise(gt_poses, [1e-3, 3e-3], [1e-3, 1e-3], seed=100, start=start_index)\n",
    "# dataset.set_odometry(odom_poses)\n",
    "\n",
    "# uncomment this part to use wrong odometry\n",
    "# T = SE3([\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 0, 1, 0],\n",
    "#     [0, 1, 0, 0],\n",
    "#     [0, 0, 0, 1]\n",
    "# ])\n",
    "# odom_poses = []\n",
    "# for i in range(1, len(gt_poses)):\n",
    "#     odom_poses += [T*gt_poses[0]]\n",
    "# dataset.set_odometry(odom_poses)\n",
    "\n",
    "def get_world_points(frame_id, sample_stride=5):\n",
    "    dataset.set_curr_index(frame_id)\n",
    "    frontend_frame = run_frontend_once(frontend)\n",
    "    color = frontend_frame.color\n",
    "    depth = frontend_frame.depth\n",
    "    sample_number = int(640*480/sample_stride/sample_stride)\n",
    "    points_xyz = np.zeros((sample_number+1, 3))\n",
    "    points_color = np.zeros((sample_number+1, 3))[::-1]\n",
    "    index = 0\n",
    "    for i in range(0,640,sample_stride):\n",
    "        for j in range(0,480,sample_stride):\n",
    "            depth = frontend_frame.depth[j, i]\n",
    "            if depth>50:\n",
    "                continue\n",
    "            points_xyz[index] = camera.back_project(i, j, depth, dataset.read_current_odometry()).flatten()\n",
    "            points_color[index] = frontend_frame.color[j, i]\n",
    "            index += 1\n",
    "    return points_xyz[:index+1], points_color[:index+1]\n",
    "\n",
    "all_points = [get_world_points(i) for i in range(100, 300, 10)]\n",
    "stacked_point_cloud = np.vstack([x[0] for x in all_points])\n",
    "stacked_point_cloud_color = np.vstack([x[1] for x in all_points])\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=stacked_point_cloud[:,2],\n",
    "    y=stacked_point_cloud[:,0],\n",
    "    z=stacked_point_cloud[:,1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color=stacked_point_cloud_color,\n",
    "    )\n",
    ")])\n",
    "\n",
    "\n",
    "# Set the aspect ratio\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    scene=dict(\n",
    "        aspectmode='data',\n",
    "        zaxis=dict(autorange=\"reversed\")\n",
    "    ),\n",
    "    scene_xaxis_title='Z',\n",
    "    scene_yaxis_title='X',\n",
    "    scene_zaxis_title='Y'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Backend\n",
    "\n",
    "In this section, we will use GTSAM library to optimize the trajectory using the measurements obtained from the frontend. To incorporate the camera pose and landmark positions in the optimization process, we will use the projection factor in GTSAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "params = Params('../params/tartanair.yaml')\n",
    "dataset = TartanAirLoader('../data/P006/')\n",
    "frontend = Frontend(params)\n",
    "backend = Backend(params)\n",
    "dataset.set_curr_index(start_index)\n",
    "n_keyframes = 200\n",
    "\n",
    "# read the ground truth and odometry\n",
    "dataset.load_ground_truth()\n",
    "gt_poses = dataset.gt\n",
    "odom_poses = dataset.add_noise(gt_poses, [1e-4, 3e-5], [1e-3, 3e-4], seed=100, start=start_index)\n",
    "dataset.set_odometry(odom_poses)\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_keyframes, 3))\n",
    "odom_traj = np.zeros((n_keyframes, 3))\n",
    "\n",
    "# run the frontend and backend\n",
    "for i in range(n_keyframes):\n",
    "    # get results from frontend\n",
    "    frontend_frame = run_frontend_once(frontend)\n",
    "    gt_traj[i] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i] = dataset.read_current_odometry().t\n",
    "    measurements = []\n",
    "    frame_id = frontend_frame.frame_id\n",
    "    count = 0\n",
    "    for landmark in frontend_frame.landmarks:\n",
    "        global_id = landmark.global_id\n",
    "        measurement = landmark.measurements[frame_id] # u, v, depth\n",
    "        measurements.append((global_id, *measurement))\n",
    "        count += 1\n",
    "    print(f'add {count} measurements to backend')\n",
    "    # add measurements to backend\n",
    "    backend.add_keyframe(frame_id, frontend_frame.odom_pose, measurements)\n",
    "# optimize the backend\n",
    "backend.optimize(optimizer='LM')\n",
    "backend_estimate = backend.current_estimate\n",
    "estimated_traj = gtsam.utilities.extractPose3(backend_estimate)[:, -3:]\n",
    "\n",
    "# plot the results\n",
    "fig = go.Figure()\n",
    "plot_trajectory(odom_traj[:n_keyframes], 'odom', fig)\n",
    "plot_trajectory(gt_traj[:n_keyframes], 'gt', fig)\n",
    "plot_trajectory(estimated_traj[:n_keyframes], 'estimated', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset.set_curr_index(start_index)\n",
    "\n",
    "# create frontend and backend\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "backend = Backend(params)\n",
    "n_keyframes = 100\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_keyframes, 3))\n",
    "odom_traj = np.zeros((n_keyframes, 3))\n",
    "\n",
    "# parameters for the backend\n",
    "initial_estimate = gtsam.Values()\n",
    "graph = gtsam.NonlinearFactorGraph()\n",
    "pose_prior_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.03]*3+[0.01]*3))\n",
    "point_noise = gtsam.noiseModel.Isotropic.Sigma(3, 0.3)\n",
    "noise_model = gtsam.noiseModel.Isotropic.Sigma(2, 1)\n",
    "noise_model_robust = gtsam.noiseModel.Robust.Create(gtsam.noiseModel.mEstimator.Huber.Create(1.345), noise_model)\n",
    "fx, fy, cx, cy = camera.camera_matrix\n",
    "K = gtsam.Cal3_S2(fx, fy, 0.0, cx, cy)\n",
    "\n",
    "total_measurements = 0\n",
    "\n",
    "# run the frontend and backend\n",
    "for i in range(n_keyframes):\n",
    "    # get results from frontend\n",
    "    frontend_frame = run_frontend_once(frontend)\n",
    "    gt_traj[i] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i] = dataset.read_current_odometry().t\n",
    "    measurements = []\n",
    "    frame_id = frontend_frame.frame_id\n",
    "    for landmark in frontend_frame.landmarks:\n",
    "        global_id = landmark.global_id\n",
    "        measurement = landmark.measurements[frame_id] # u, v, depth\n",
    "        measurements.append((global_id, *measurement))\n",
    "\n",
    "    # add measurements to backend\n",
    "    odom_pose = frontend_frame.odom_pose\n",
    "    pose = gtsam.Pose3(gtsam.Rot3(odom_pose.R), gtsam.Point3(odom_pose.t))\n",
    "    # add initial estimate\n",
    "    initial_estimate.insert(X(frame_id), pose)\n",
    "    if i == 0:\n",
    "        # add prior for first frame\n",
    "        graph.add(gtsam.PriorFactorPose3(X(frame_id), pose, pose_prior_noise))\n",
    "    count = 0\n",
    "    for global_id, u, v, depth in measurements:\n",
    "        # add measurements\n",
    "        global_pos = camera.back_project(u, v, depth, odom_pose).flatten()\n",
    "        if initial_estimate.exists(L(global_id)):\n",
    "            init_pos = initial_estimate.atVector(L(global_id))\n",
    "            # calc error\n",
    "            error = np.linalg.norm(init_pos - global_pos)\n",
    "            if error>0.5:\n",
    "                continue\n",
    "        graph.add(gtsam.GenericProjectionFactorCal3_S2(\n",
    "            np.array([u, v]),\n",
    "            noise_model_robust,\n",
    "            X(frame_id),\n",
    "            L(global_id),\n",
    "            K\n",
    "        ))\n",
    "        count += 1\n",
    "        if total_measurements<50:\n",
    "            total_measurements += 1\n",
    "            graph.add(gtsam.PriorFactorPoint3(L(global_id), global_pos, point_noise))\n",
    "        if not initial_estimate.exists(L(global_id)):\n",
    "            initial_estimate.insert(L(global_id), global_pos)\n",
    "    print(f'add {count} measurements to backend')\n",
    "# optimize\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial_estimate)\n",
    "current_estimate = optimizer.optimize()\n",
    "estimated_traj = gtsam.utilities.extractPose3(current_estimate)[:, -3:]\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig = go.Figure()\n",
    "plot_trajectory(odom_traj[:n_keyframes], 'odom', fig)\n",
    "plot_trajectory(gt_traj[:n_keyframes], 'gt', fig)\n",
    "plot_trajectory(estimated_traj[:n_keyframes], 'estimated', fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Loop Closure\n",
    "\n",
    "In this section, we will explore the concept of loop closure by building a toy example.\n",
    "\n",
    "We will be using the visual bag of word method, similar to ORB-SLAM2, to detect a loop in the trajectory. However, since there are no well-built visual bag of word libraries in Python, we will assume that we have successfully found a loop. To have a more complete implementation, you can refer to [nicolov/simple_slam_loop_closure](https://github.com/nicolov/simple_slam_loop_closure/blob/master/src/new_college.cpp). Additionally, we will build a pose graph in this example to simplify the calculation and demonstrate how the loop closure affects the estimated trajectory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The loop\n",
    "\n",
    "First, we will visualize the trajectory which includes a loop. It's important to note that a loop doesn't necessarily mean the robot comes back to the exact same place, but rather it should be near a place it visited before so that a relative pose between two frames can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load seq P006\n",
    "dataset = TartanAirLoader('../data/P006/')\n",
    "gt_poses = dataset._load_traj('tum', traj_filename, add_timestamps=True)\n",
    "gt_traj = np.array([x.t for x in gt_poses])\n",
    "\n",
    "# set the start and end frame for the sequence\n",
    "seq_start = 185\n",
    "seq_end = 518\n",
    "n_frames = seq_end - seq_start\n",
    "\n",
    "# draw the loop\n",
    "plt.clf()\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.plot(gt_traj[seq_start:seq_end,0], gt_traj[seq_start:seq_end,1], gt_traj[seq_start:seq_end,2], label='gt')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start and end frame of the loop should have a similar viewpoint and share a significant number of visual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_curr_index(seq_start)\n",
    "color_start, _ = dataset.read_current_rgbd()\n",
    "dataset.set_curr_index(seq_end)\n",
    "color_end, _ = dataset.read_current_rgbd()\n",
    "plt.clf()\n",
    "plt.imshow(np.hstack((color_start, color_end)))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The drift\n",
    "\n",
    "In practice, the odometry estimates can contain noise and drift due to various factors such as sensor noise and model inaccuracies. To simulate such scenarios, we can add some noise to the ground truth odometry and use it as the noisy odometry. In the figure below, we visualize both the ground truth and noisy odometry trajectories. Before we add loop closure, the estimated trajectory from GTSAM will be identical to the noisy odometry trajectory. It is important to note that due to the accumulated error in each frame, the start and end positions of the loop may differ significantly from the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_poses = dataset.add_noise(gt_poses[seq_start:], [3e-3, 5e-4], [3e-4, 3e-4], seed=100)\n",
    "odom_poses = SE3([*gt_poses[:seq_start], *noisy_poses])\n",
    "dataset.set_odometry(odom_poses)\n",
    "dataset.set_ground_truth(gt_poses)\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_frames, 3))\n",
    "odom_traj = np.zeros((n_frames, 3))\n",
    "\n",
    "# add the odometry poses to the graph\n",
    "for i in range(seq_start, seq_end):\n",
    "    dataset.set_curr_index(i)\n",
    "    gt_traj[i-seq_start] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i-seq_start] = dataset.read_current_odometry().t\n",
    "\n",
    "plt.clf()\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.plot(odom_traj[:,0],odom_traj[:,1],odom_traj[:,2], '--', label='odom')\n",
    "ax.plot(gt_traj[:,0],gt_traj[:,1],gt_traj[:,2], '--', label='gt')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add loop closure constraint\n",
    "\n",
    "To add the loop closure constraint, we first need to detect the loop, which is usually done using a visual bag-of-words approach. This involves extracting features from each frame and then creating a dictionary of visual words to represent them. Then, we can match the visual words between frames to create a graph of similar frames, where each node represents a frame and edges represent similar frames.\n",
    "\n",
    "Once we have detected the loop and found the relative pose between the start and end frame using algorithm like [PnP](https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html), we can add a loop closure constraint to the pose graph optimization. As a result, the estimated trajectory will be much closer to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gtsam_pose(pose):\n",
    "    return gtsam.Pose3(gtsam.Rot3(pose.R), gtsam.Point3(pose.t))\n",
    "\n",
    "# initialize the backend\n",
    "graph = gtsam.NonlinearFactorGraph()\n",
    "initial_estimate = gtsam.Values()\n",
    "\n",
    "# add prior for the first pose\n",
    "graph.push_back(gtsam.PriorFactorPose3(\n",
    "    X(seq_start), to_gtsam_pose(odom_poses[seq_start]), gtsam.noiseModel.Diagonal.Sigmas(np.ones(6)*0.0001)\n",
    "))\n",
    "initial_estimate.insert(X(seq_start), to_gtsam_pose(odom_poses[seq_start]))\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_frames, 3))\n",
    "odom_traj = np.zeros((n_frames, 3))\n",
    "\n",
    "# add the odometry poses to the graph\n",
    "for i in range(seq_start, seq_end):\n",
    "    dataset.set_curr_index(i)\n",
    "    gt_traj[i-seq_start] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i-seq_start] = dataset.read_current_odometry().t\n",
    "    initial_estimate.insert(X(i+1), to_gtsam_pose(odom_poses[i]))\n",
    "    graph.push_back(gtsam.BetweenFactorPose3(\n",
    "        X(i), X(i+1), to_gtsam_pose(odom_poses[i].inv()*odom_poses[i+1]),\n",
    "        gtsam.noiseModel.Diagonal.Sigmas(np.ones(6)*0.5) # we set a higher noise for odometry\n",
    "    ))\n",
    "\n",
    "# add loop closures\n",
    "graph.push_back(gtsam.BetweenFactorPose3(\n",
    "    X(seq_end), X(seq_start), to_gtsam_pose(gt_poses[seq_end].inv()*gt_poses[seq_start]),\n",
    "    gtsam.noiseModel.Diagonal.Sigmas(np.ones(6)*0.001)  # we set a lower noise for loop closure constraints\n",
    "))\n",
    "\n",
    "# optimize the graph\n",
    "print('before optimizaiton error: ', graph.error(initial_estimate))\n",
    "optimizer_params = gtsam.LevenbergMarquardtParams()\n",
    "optimizer_params = gtsam.LevenbergMarquardtParams.CeresDefaults()\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial_estimate, optimizer_params)\n",
    "current_estimate = optimizer.optimize()\n",
    "estimated_traj = gtsam.utilities.extractPose3(current_estimate)[:, -3:]\n",
    "print('after optimizaiton error: ', graph.error(current_estimate))\n",
    "\n",
    "plt.clf()\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.plot(odom_traj[:,0],odom_traj[:,1],odom_traj[:,2], '--', label='odom')\n",
    "ax.plot(gt_traj[:,0],gt_traj[:,1],gt_traj[:,2], '--', label='gt')\n",
    "ax.plot(estimated_traj[:,0],estimated_traj[:,1], estimated_traj[:,2], label='estimated')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have gained a solid understanding of the basic concepts and techniques used in real SLAM systems. By following the steps and examples we provided, you now have the knowledge to implement your own SLAM system in Python. Keep in mind that SLAM is a complex and evolving field, and there is always more to learn and explore. Don't hesitate to continue your learning journey and discover more advanced techniques and applications in SLAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

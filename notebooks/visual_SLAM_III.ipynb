{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual SLAM Trilogy\n",
    "## Part III: loop Closure\n",
    "\n",
    "In this SLAM hands-on lecture, we will implement a visual SLAM system that has an OpenCV frontend, a GTSAM backend, and a loop closure module based on a bag-of-words approach.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The overview of our SLAM system is depicted below, simplified by certain assumptions:\n",
    "\n",
    "1. Odometry\n",
    "    - Assumes there is an odometry trajectory provided to our SLAM system.\n",
    "    - In practice, this includes a Kalman filter fusing the IMU and encoder data.\n",
    "2. Frontend:\n",
    "    - Processes the raw sensor data and extracts relevant features for optimization.\n",
    "    - Associates each measurement to a specific landmark (3D point).\n",
    "    - Provide initial values for the backend variables.\n",
    "3. Mapping\n",
    "    - Utilizes a very minimum sparse map.\n",
    "    - Could be replaced with OGM or even 3D Gaussian Splatting in the future.\n",
    "4. Backend\n",
    "    - Solve the maximum a posteriori (MAP) estimation problem.\n",
    "    - Feed back information to loop closure.\n",
    "5. Loop closure:\n",
    "    - Acts as a long-term tracking module (compared to the short-term tracking module in frontend).\n",
    "    - Implemented with visual bag-of-word algorithm.\n",
    "\n",
    "![slam_overview](assets/slam_overview.png)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the abandoned_factory P006 sequence from the TartanAir dataset to test the system. It is a simulation dataset with diverse environments and ground truth dataset, which make it perfect for testing and evaluating our system. To get started, we'll need to access the camera intrinsics, extrinsics, and data format information, which can be found here: https://github.com/castacks/tartanair_tools/blob/master/data_type.md.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "In this notebook, we will explore the concept of loop closure by building a toy example. Specifically, we will cover the following topics:\n",
    "\n",
    "- Detect the loop by calculating the confusion matrix using Bag of Word (BoW) algorithm\n",
    "- Optimize the trajectory with GTSAM\n",
    "- Future readings about different categories of visual SLAMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Dependency\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the minslam package in “editable” mode\n",
    "# !pip install -e ..\n",
    "\n",
    "# install other libraries\n",
    "# !pip install numpy spatialmath-python opencv-python matplotlib gtsam ipympl evo plotly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import libraries and dataset\n",
    "Please download [abadoned_factory P006 dataset](https://drive.google.com/file/d/1Q_fSI0U-IMfv90lyE1Uh78KV2QJheHbv/view?usp=share_link) and extract it to a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block should run without error\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "\n",
    "# change the perspective of 3d plots with this command\n",
    "%matplotlib widget\n",
    "\n",
    "# test if we can find the dataset\n",
    "dataset_folder = '/home/junzhewu/Projects/minimum-slam/data/tartanair/scenes/abandonedfactory/Easy/P006'\n",
    "print('Check if the data folder exists:',os.path.exists(dataset_folder))\n",
    "\n",
    "# visualization\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# frontend\n",
    "import numpy as np\n",
    "from spatialmath import *\n",
    "\n",
    "# backend\n",
    "import gtsam\n",
    "from gtsam.symbol_shorthand import L, X\n",
    "\n",
    "# our slam implementation\n",
    "from minslam.data_loader import TartanAirLoader, plot_trajectory\n",
    "from minslam.frontend import Frontend\n",
    "from minslam.params import Params\n",
    "from minslam.backend import Backend\n",
    "from minslam.camera import PinholeCamera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Loop Closure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Detect Loop\n",
    "\n",
    "We will be using the visual bag of word method, similar to ORB-SLAM2, to detect a loop in the trajectory. However, since there are no well-built visual bag of word libraries in Python, we will use C++ to call visual bag of word and compute the confusion matrix, and then visualize the confusion matrix in this notebook. For the C++ loop closure detection implementation, you can refer to [tccoin/simple_slam_loop_closure](https://github.com/tccoin/simple_slam_loop_closure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain the confusion matrix from the C++ algorithm, we should visualize it. Each pixel in the matrix represents the similarity between frames i and j. Higher pixel values indicate a greater likelihood of a loop closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.loadtxt('confusion_tartanair_af_P006.txt', delimiter=',')\n",
    "frame_interval = 1\n",
    "N = confusion.shape[0]\n",
    "# plot\n",
    "fig = px.imshow(confusion, color_continuous_scale='gray')\n",
    "fig.update_layout(title='Confusion Matrix', width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values along the diagonal of the matrix are significantly high, which is expected as adjacent frames typically appear similar. This information is already leveraged in the local pose estimation phase, where feature tracking helps optimize landmark positions and poses. However, the global loop closures we aim to detect should involve non-adjacent frames. Therefore, we will only consider loop closure candidates that are separated by at least 100 frames to ensure they are truly global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loop_interval = 100\n",
    "confusion_filtered = confusion.copy()\n",
    "for i in range(N):\n",
    "    confusion_filtered[i:i+int(min_loop_interval/frame_interval), i] = 0\n",
    "# plot\n",
    "fig = px.imshow(confusion_filtered, color_continuous_scale='gray')\n",
    "fig.update_layout(title='Filtered Confusion Matrix', width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will evaluate each loop closure candidate by identifying visual feature matches between two frames.\n",
    "\n",
    "This step could be omitted if we enhance the accuracy of the bag of words algorithm. For instance, refining the vocabulary file by training it on the TartanAir dataset could lead to improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each frame, add a new candidate if the max confidence is above 0.015\n",
    "loop_candidates = []\n",
    "for i in range(N):\n",
    "    j = np.argmax(confusion_filtered[i])\n",
    "    if confusion_filtered[i, j] > 0.015:\n",
    "        loop_candidates.append((i,j))\n",
    "print('number of loop candidates:', len(loop_candidates))\n",
    "\n",
    "# load dataset\n",
    "dataset = TartanAirLoader(dataset_folder)\n",
    "dataset.load_ground_truth()\n",
    "gt_traj = np.array([x.t for x in dataset.gt])\n",
    "\n",
    "# check visual similarity\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "matches_number = []\n",
    "for i,j in loop_candidates:\n",
    "    # calculate the number of matches for each candidate\n",
    "    dataset.set_curr_index(i)\n",
    "    pose = dataset.read_current_ground_truth()\n",
    "    color, depth = dataset.read_current_rgbd()\n",
    "    frontend.add_keyframe(pose, color, depth, dataset.curr_index)\n",
    "    frontend.extract_features()\n",
    "    dataset.set_curr_index(j)\n",
    "    pose = dataset.read_current_ground_truth()\n",
    "    color, depth = dataset.read_current_rgbd()\n",
    "    frontend.add_keyframe(pose, color, depth, dataset.curr_index)\n",
    "    frontend.extract_features()\n",
    "    frontend.match_features()\n",
    "    matches_number.append(len(frontend.curr_frame.matches))\n",
    "# choose the candidate with the most matches\n",
    "max_matches_idx = np.argmax(matches_number)\n",
    "print('candidate with the most matches:', loop_candidates[max_matches_idx])\n",
    "print('number of matches:', matches_number[max_matches_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The loop\n",
    "\n",
    "First, we will visualize the trajectory which includes a loop. It's important to note that a loop doesn't necessarily mean the robot comes back to the exact same place, but rather it should be near a place it visited before so that a relative pose between two frames can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load seq P006\n",
    "dataset = TartanAirLoader(dataset_folder)\n",
    "dataset.load_ground_truth()\n",
    "gt_traj = np.array([x.t for x in dataset.gt])\n",
    "\n",
    "# set the start and end frame for the sequence\n",
    "seq_start = 185\n",
    "seq_end = 519\n",
    "n_frames = seq_end - seq_start\n",
    "\n",
    "# plot the loop\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type':'scatter3d'}]])\n",
    "fig.add_trace(go.Scatter3d(x=gt_traj[seq_start:seq_end,0], y=gt_traj[seq_start:seq_end,1], z=gt_traj[seq_start:seq_end,2], mode='lines', name='gt'))\n",
    "fig.update_layout(scene=dict(aspectmode='cube'))\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start and end frame of the loop should have a similar viewpoint and share a significant number of visual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_curr_index(seq_start)\n",
    "color_start, _ = dataset.read_current_rgbd()\n",
    "dataset.set_curr_index(seq_end)\n",
    "color_end, _ = dataset.read_current_rgbd()\n",
    "image = np.hstack([color_start, color_end])\n",
    "\n",
    "# plot using plotly\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type':'image'}]])\n",
    "fig.add_trace(go.Image(z=image))\n",
    "fig.update_layout(scene=dict(aspectmode='cube'))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The drift\n",
    "\n",
    "In practice, the odometry estimates can contain noise and drift due to various factors such as sensor noise and model inaccuracies. To simulate such scenarios, we can add some noise to the ground truth odometry and use it as the noisy odometry. In the figure below, we visualize both the ground truth and noisy odometry trajectories. Before we add loop closure, the estimated trajectory from GTSAM will be identical to the noisy odometry trajectory. It is important to note that due to the accumulated error in each frame, the start and end positions of the loop may differ significantly from the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_poses = dataset.gt\n",
    "noisy_poses = dataset.add_noise(gt_poses[seq_start:], [3e-4, 3e-4], [1e-3, 3e-4], seed=530)\n",
    "odom_poses = SE3([*gt_poses[:seq_start], *noisy_poses])\n",
    "dataset.set_odometry(odom_poses)\n",
    "dataset.set_ground_truth(gt_poses)\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_frames, 3))\n",
    "odom_traj = np.zeros((n_frames, 3))\n",
    "\n",
    "# add the odometry poses to the graph\n",
    "for i in range(seq_start, seq_end):\n",
    "    dataset.set_curr_index(i)\n",
    "    gt_traj[i-seq_start] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i-seq_start] = dataset.read_current_odometry().t\n",
    "\n",
    "# plot the trajectories\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type':'scatter3d'}]])\n",
    "fig.add_trace(go.Scatter3d(x=odom_traj[:,0], y=odom_traj[:,1], z=odom_traj[:,2], mode='lines', name='odom'))\n",
    "fig.add_trace(go.Scatter3d(x=gt_traj[:,0], y=gt_traj[:,1], z=gt_traj[:,2], mode='lines', name='gt'))\n",
    "fig.update_layout(scene=dict(aspectmode='cube'))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loop Closure Optimization\n",
    "\n",
    "The goal is to build a pose graph and conduct the global pose optimization. To simplify the calculation, we will not add the visual landmarks.\n",
    "\n",
    "To add the loop closure constraint, we first need to detect the loop, which is usually done using a visual bag-of-words approach. This involves extracting features from each frame and then creating a dictionary of visual words to represent them. Then, we can match the visual words between frames to create a graph of similar frames, where each node represents a frame and edges represent similar frames.\n",
    "\n",
    "Once we have detected the loop and found the relative pose between the start and end frame using algorithm like [PnP](https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html), we can add a loop closure constraint to the pose graph optimization. As a result, the estimated trajectory will be much closer to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gtsam_pose(pose):\n",
    "    return gtsam.Pose3(gtsam.Rot3(pose.R), gtsam.Point3(pose.t))\n",
    "\n",
    "# initialize the backend\n",
    "graph = gtsam.NonlinearFactorGraph()\n",
    "initial_estimate = gtsam.Values()\n",
    "\n",
    "# add prior for the first pose\n",
    "graph.push_back(gtsam.PriorFactorPose3(\n",
    "    X(seq_start), to_gtsam_pose(odom_poses[seq_start]), gtsam.noiseModel.Diagonal.Sigmas(np.ones(6)*0.0001)\n",
    "))\n",
    "initial_estimate.insert(X(seq_start), to_gtsam_pose(odom_poses[seq_start]))\n",
    "\n",
    "# for plotting\n",
    "gt_traj = np.zeros((n_frames, 3))\n",
    "odom_traj = np.zeros((n_frames, 3))\n",
    "\n",
    "# add the odometry poses to the graph\n",
    "for i in range(seq_start, seq_end):\n",
    "    dataset.set_curr_index(i)\n",
    "    gt_traj[i-seq_start] = dataset.read_current_ground_truth().t\n",
    "    odom_traj[i-seq_start] = dataset.read_current_odometry().t\n",
    "    initial_estimate.insert(X(i+1), to_gtsam_pose(odom_poses[i]))\n",
    "    graph.push_back(gtsam.BetweenFactorPose3(\n",
    "        X(i), X(i+1), to_gtsam_pose(odom_poses[i].inv()*odom_poses[i+1]),\n",
    "        gtsam.noiseModel.Diagonal.Sigmas(np.ones(6)*0.5) # we set a higher noise for odometry\n",
    "    ))\n",
    "\n",
    "# add loop closures\n",
    "graph.push_back(gtsam.BetweenFactorPose3(\n",
    "    X(seq_end), X(seq_start), to_gtsam_pose(gt_poses[seq_end].inv()*gt_poses[seq_start]),\n",
    "    gtsam.noiseModel.Diagonal.Sigmas(np.ones(6)*0.01)  # we set a lower noise for loop closure constraints\n",
    "))\n",
    "# TODO: try to set it to a higher value and see what happens\n",
    "\n",
    "# optimize the graph\n",
    "print('before optimizaiton error: ', graph.error(initial_estimate))\n",
    "optimizer_params = gtsam.LevenbergMarquardtParams()\n",
    "optimizer_params = gtsam.LevenbergMarquardtParams.CeresDefaults()\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, initial_estimate, optimizer_params)\n",
    "current_estimate = optimizer.optimize()\n",
    "estimated_traj = gtsam.utilities.extractPose3(current_estimate)[:, -3:]\n",
    "print('after optimizaiton error: ', graph.error(current_estimate))\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type':'scatter3d'}]])\n",
    "fig.add_trace(go.Scatter3d(x=odom_traj[:,0], y=odom_traj[:,1], z=odom_traj[:,2], mode='lines', name='odom'))\n",
    "fig.add_trace(go.Scatter3d(x=gt_traj[:,0], y=gt_traj[:,1], z=gt_traj[:,2], mode='lines', name='gt'))\n",
    "fig.add_trace(go.Scatter3d(x=estimated_traj[:,0], y=estimated_traj[:,1], z=estimated_traj[:,2], mode='lines', name='estimated'))\n",
    "fig.update_layout(scene=dict(aspectmode='cube'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Visual SLAM Categories\n",
    "\n",
    "- **Feature-Based SLAM** detects and tracks distinct features within the environment.\n",
    "    - Example: [ORB-SLAM 3](https://github.com/UZ-SLAMLab/ORB_SLAM3)\n",
    "\n",
    "- **Direct SLAM** directly uses the intensity values of all pixels in an image to estimate the camera motion and structure of the environment. This method is usually more sensitive to lighting variations but can work well in low-texture environments where feature-based method struggles.\n",
    "    - Example: [LSD-SLAM](https://github.com/tum-vision/lsd_slam)\n",
    "\n",
    "- **Dense SLAM** creates a dense reconstruction of the environment using either stereo cameras or RGBD cameras. This method involves processing a larger amount of data to produce a more detailed map.\n",
    "    - Example: [KinectFusion](https://github.com/ParikaGoel/KinectFusion), [ElasticFusion](https://github.com/mp3guy/ElasticFusion)\n",
    "\n",
    "- **Semantic SLAM** incorporates machine learning techniques to understand and label different parts of the environment based on their meaning.\n",
    "    - Example: [Khronos](https://github.com/MIT-SPARK/Khronos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Conclusion\n",
    "Congratulations! You have gained a solid understanding of the basic concepts and techniques used in real SLAM systems. By following the steps and examples we provided, you now have the knowledge to implement your own SLAM system in Python. Keep in mind that SLAM is a complex and evolving field, and there is always more to learn and explore. Don't hesitate to continue your learning journey and discover more advanced techniques and applications in SLAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "na568",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual SLAM Trilogy\n",
    "## Part I: The Frontend\n",
    "\n",
    "~~In this SLAM hands-on lecture, we will implement a visual SLAM system that has an OpenCV frontend, a GTSAM backend, and a loop closure module based on a bag-of-words approach.~~\n",
    "\n",
    "In a land where visions intertwine with ancient algorithms, we embark on a quest to forge a **visual SLAM system**. With the wisdom of **OpenCV** and the deep magics of **GTSAM** at our command, we shall weave a loop closure module, crafted from the mystical **bag-of-words** algorithm. This noble endeavor calls upon the brave to unlock the secrets of perception, guiding our creation through the unseen paths of the world.\n",
    "\n",
    "![lord_of_slam](assets/lord_of_slam.webp)\n",
    "\n",
    "## Now, back to reality\n",
    "\n",
    "## Overview\n",
    "\n",
    "The overview of our SLAM system is depicted below, simplified by certain assumptions:\n",
    "\n",
    "1. Odometry\n",
    "    - Assumes there is an odometry trajectory provided to our SLAM system.\n",
    "    - In practice, this includes a Kalman filter fusing the IMU and encoder data.\n",
    "2. Frontend:\n",
    "    - Processes the raw sensor data and extracts relevant features for optimization.\n",
    "    - Associates each measurement to a specific landmark (3D point).\n",
    "    - Provide initial values for the backend variables.\n",
    "3. Mapping\n",
    "    - Utilizes a very minimum sparse map.\n",
    "    - Could be replaced with OGM or even 3D Gaussian Splatting in the future.\n",
    "4. Backend\n",
    "    - Solve the maximum a posteriori (MAP) estimation problem.\n",
    "    - Feed back information to loop closure.\n",
    "5. Loop closure:\n",
    "    - Acts as a long-term tracking module (compared to the short-term tracking module in frontend).\n",
    "    - Implemented with visual bag-of-word algorithm.\n",
    "\n",
    "![slam_overview](assets/slam_overview.png)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the abandoned_factory P006 sequence from the TartanAir dataset to test the system. It is a simulation dataset with diverse environments and ground truth dataset, which make it perfect for testing and evaluating our system. To get started, we'll need to access the camera intrinsics, extrinsics, and data format information, which can be found here: https://github.com/castacks/tartanair_tools/blob/master/data_type.md.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "In this notebook, we will walk through the implementation of the frontend step-by-step, while visualizing the output of each step. Specifically, we will cover the following topics:\n",
    "\n",
    "- Loading the dataset\n",
    "- Selecting keyframes\n",
    "- Extracting features and tracking them across frames\n",
    "- Removing outlier matches\n",
    "- Assigning global IDs to features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Dependency\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Python libraries\n",
    "\n",
    "Please use python>=3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install the minslam package in “editable” mode\n",
    "# !pip install -e ..\n",
    "\n",
    "# # install other libraries\n",
    "# !pip install numpy spatialmath-python opencv-python matplotlib gtsam ipympl evo plotly nbformat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import libraries and dataset\n",
    "Please download [abadoned_factory P006 dataset](https://drive.google.com/file/d/1Q_fSI0U-IMfv90lyE1Uh78KV2QJheHbv/view?usp=share_link) and extract it to a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block should run without error\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "\n",
    "# test if we can find the dataset\n",
    "dataset_folder = '../data/tartanair/scenes/abandonedfactory/Easy/P006'\n",
    "print('Check if the data folder exists:',os.path.exists(dataset_folder))\n",
    "\n",
    "# visualization\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# frontend\n",
    "import numpy as np\n",
    "from spatialmath import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# backend\n",
    "import gtsam\n",
    "from gtsam.symbol_shorthand import L, X\n",
    "\n",
    "# our slam implementation\n",
    "from minslam.data_loader import TartanAirLoader, plot_trajectory\n",
    "from minslam.frontend import Frontend\n",
    "from minslam.params import Params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Frontend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load images and trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_filename = 'pose_left.txt'\n",
    "traj_path = os.path.join(dataset_folder, traj_filename)\n",
    "print('Loading trajectory from ', traj_path)\n",
    "\n",
    "# Have a look at the trajectory file\n",
    "print('First 3 lines of TartanAir trajectory file:')\n",
    "with open(traj_path, 'r') as f:\n",
    "    print(''.join(f.readlines()[:3])) # tx ty tz qx qy qz qw\n",
    "\n",
    "# load a trajectory\n",
    "print('First 3 SE3 poses:')\n",
    "dataset = TartanAirLoader(dataset_folder)\n",
    "gt_poses = dataset._load_traj('tum', traj_filename, add_timestamps=True)\n",
    "dataset.set_ground_truth(gt_poses)\n",
    "print(gt_poses[:3])\n",
    "\n",
    "# add noise to the gt and set it as odometry\n",
    "odom_poses = dataset.add_noise(gt_poses, [1e-4, 3e-4], [1e-3, 1e-3], seed=100)\n",
    "dataset.set_odometry(gt_poses)\n",
    "\n",
    "# plot the trajectory in 3d\n",
    "gt_traj = np.array([p.t for p in gt_poses])\n",
    "odom_traj = np.array([p.t for p in odom_poses])\n",
    "fig = go.Figure()\n",
    "n_keyframes = 300\n",
    "plot_trajectory(odom_traj[:n_keyframes], 'odom', fig)\n",
    "plot_trajectory(gt_traj[:n_keyframes], 'gt', fig)\n",
    "fig.show()\n",
    "\n",
    "# load the first frame\n",
    "dataset.set_curr_index(50)\n",
    "color, depth = dataset.read_current_rgbd()\n",
    "\n",
    "# show color and depth horizontally\n",
    "print('color image data type:', color.dtype)\n",
    "print('depth image data type:', depth.dtype)\n",
    "print('depth image range:', f'{depth.min()} - {depth.max()}')\n",
    "\n",
    "fig_color = px.imshow(color[:,:,::-1])\n",
    "fig_color.update_traces(hoverinfo=\"x+y+z\", name=\"\")\n",
    "\n",
    "clipped_depth = depth.clip(0, 40)\n",
    "fig_depth = px.imshow(clipped_depth, color_continuous_scale='gray')\n",
    "fig_depth.update_traces(hoverinfo=\"x+y+z\", name=\"\")\n",
    "\n",
    "fig_color.show()\n",
    "fig_depth.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Keyframe selection\n",
    "\n",
    "Instead of processing every incoming frame, we choose to pick some \"keyframes\" to reduce computation. In this step, we need to ensure sufficient transform for landmark triangulation. To do this, we define the distance between two odometry poses as $\\lVert Log (X_{i}^{-1} X_{i+1}) \\rVert$, and if this distance is greater than a specified threshold, we add the new frame as a keyframe.\n",
    "\n",
    "The figure below illustrates the effect of increasing the threshold on the number of keyframes we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize our frontend implementation\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "\n",
    "# The keyframe_selection function accepts a pose and returns a boolean.\n",
    "# The first frame is always a keyframe, then we check if the motion is\n",
    "# large enough.\n",
    "pose = dataset.read_current_odometry()\n",
    "is_keyframe = frontend.keyframe_selection(pose)\n",
    "\n",
    "# What if we set the threshold to different values?\n",
    "traces = []\n",
    "for threshold in np.arange(0, 1.1, 0.2):\n",
    "    frontend.params['frontend']['keyframe']['threshold'] = threshold\n",
    "    keyframe_selections = np.zeros(100)\n",
    "    for i in range(100):\n",
    "        dataset.set_curr_index(i)\n",
    "        pose = dataset.read_current_odometry()\n",
    "        keyframe_selections[i] = frontend.keyframe_selection(pose)\n",
    "        if keyframe_selections[i]:\n",
    "            frontend.add_keyframe(pose, color, depth)\n",
    "    \n",
    "    # Generating x values (frame IDs where keyframes are selected)\n",
    "    x_vals = np.arange(100)[keyframe_selections == 1]\n",
    "    y_vals = np.full(x_vals.shape, threshold) # Y value is constant as the threshold\n",
    "\n",
    "    # Add a trace for each threshold\n",
    "    traces.append(go.Scatter(x=x_vals, y=y_vals, mode='markers', name=f'threshold={round(threshold, 1)}',\n",
    "                             hoverinfo='text', text=['Frame ID: %d' % i for i in x_vals]))\n",
    "\n",
    "# Create the figure with all traces\n",
    "fig = go.Figure(data=traces)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Keyframe Selection Threshold Analysis',\n",
    "                  xaxis_title='Frame ID',\n",
    "                  yaxis_title='Threshold',\n",
    "                  height=600, width=800)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract features and generate matches\n",
    "\n",
    "To detect and describe features in the image, we'll be using the [Scale-Invariant Feature Transform (SIFT)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). Other options such as ORB, FAST, and AKAZE are also available. To track the features, we could use a brute-force matcher to compare each pair of feature descriptors and choose the best matches. Another method is using [optical flow](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html) that tracks how image patches (3x3) move across different frames. This method provides better tracking results and more consistent runtime when the movement is small.\n",
    "\n",
    "Here we are using the brute-force matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear previous states\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "\n",
    "# add a keyframe\n",
    "dataset.set_curr_index(100)\n",
    "pose = dataset.read_current_odometry()\n",
    "color, depth = dataset.read_current_rgbd()\n",
    "frontend.add_keyframe(pose, color, depth)\n",
    "\n",
    "# extract features\n",
    "frontend.extract_features()\n",
    "frontend.assign_global_id()\n",
    "fig = frontend.plot_features()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another keyframe\n",
    "dataset.set_curr_index(150)\n",
    "pose = dataset.read_current_odometry()\n",
    "color, depth = dataset.read_current_rgbd()\n",
    "frontend.add_keyframe(pose, color, depth)\n",
    "\n",
    "# match features\n",
    "frontend.extract_features()\n",
    "frontend.match_features('bruteforce')\n",
    "fig = frontend.plot_matches(plot_id=False)\n",
    "fig.show()\n",
    "\n",
    "print('number of matches before outlier rejection:', len(frontend.curr_frame.matches))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove match outliers\n",
    "\n",
    "To remove incorrect matches, we'll be using [`cv2.findFundamentalMat`](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a). This function uses the [epipolar geometry model](https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf) to describe the relationship between matches, based on the fundamental matrix. It then detects outliers based on how well a match fits the model.\n",
    "\n",
    "![epipolar geometry](assets/epipolar%20geometry.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after removing outliers, the wrong match we added should be removed\n",
    "frontend.eliminate_outliers()\n",
    "fig = frontend.plot_matches(plot_id=False)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Assign global ID\n",
    "\n",
    "Then, for each tracked feature, we assign a global id to it. The global id won't change across the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear previous states\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "\n",
    "# add a keyframe\n",
    "dataset.set_curr_index(100)\n",
    "pose = dataset.read_current_odometry()\n",
    "color, depth = dataset.read_current_rgbd()\n",
    "frontend.add_keyframe(pose, color, depth)\n",
    "frontend.extract_features()\n",
    "frontend.assign_global_id()\n",
    "\n",
    "# add another keyframe\n",
    "dataset.set_curr_index(150)\n",
    "pose = dataset.read_current_odometry()\n",
    "color, depth = dataset.read_current_rgbd()\n",
    "frontend.add_keyframe(pose, color, depth)\n",
    "frontend.extract_features()\n",
    "frontend.match_features()\n",
    "frontend.eliminate_outliers()\n",
    "frontend.assign_global_id()\n",
    "\n",
    "fig = frontend.plot_matches(plot_id=True)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test the frontend\n",
    "\n",
    "Finally, we can construct a working frontend!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear previous states\n",
    "params = Params('../params/tartanair.yaml')\n",
    "frontend = Frontend(params)\n",
    "dataset.set_curr_index(100)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(np.zeros([480, 1280, 3]))\n",
    "\n",
    "# run the whole pipeline once\n",
    "def run_once(frame_num):\n",
    "\n",
    "    pose = dataset.read_current_odometry()\n",
    "    while not frontend.keyframe_selection(pose):\n",
    "        if not dataset.load_next_frame():\n",
    "            break\n",
    "        pose = dataset.read_current_odometry()\n",
    "    color, depth = dataset.read_current_rgbd()\n",
    "    frontend.add_keyframe(pose, color, depth)\n",
    "    print(f'--- Added keyframe {frontend.frame_id} (seq id: {dataset.curr_index}) ---')\n",
    "    frontend.extract_features()\n",
    "    if frontend.frame_id > 0:\n",
    "        frontend.match_features()\n",
    "        frontend.eliminate_outliers()\n",
    "    frontend.assign_global_id()\n",
    "    # do not show the plot\n",
    "    plt.ioff()\n",
    "\n",
    "    img = frontend.plot_matches(fig=fig, plot_id=True, matplot=True)\n",
    "    im.set_data(img)\n",
    "    return [im]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate tracking animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "anim = FuncAnimation(fig, run_once, frames=100)\n",
    "anim.save('recitation_tracking.mp4', writer='ffmpeg', fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "na568",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
